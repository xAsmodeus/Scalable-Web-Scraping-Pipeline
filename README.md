                                         ** Runned and Tested in Google Colab **                                           
## ðŸŽ¯ Context

Internship challenge from Digiole. The task was to create, build and run a **modular scraping pipeline** that gathers verified business contact data (especially emails) from publicly available directories across selected industries.

Tasks: 
- Extract company page links from a directory.
- Visit those links to collect email addresses.
- Clean, structure, and store the results in CSV files.

<img width="675" height="248" alt="image" src="https://github.com/user-attachments/assets/c0903703-ef57-4ced-a29b-aef82ffabd81" />
## Approach

### ðŸš€ Challenge Prompt

> â€œImagine youâ€™re tasked with generating a contact database for wine producers in Europe, starting with data from [Europages](https://www.europages.co.uk/). How would you approach this challenge in a structured, repeatable, and scalable way?â€
> 

## Difficulties
1) Some webpages didn't own a 'Visit Webpage' button so an early approach resulted to having some slots empty in the email CSV.
2) To "route" to different webpages
3) For example: "Green Life Revolution sl" didn't have a

## References
1. https://www.notion.so/digiole/Scalable-Web-Scraping-Pipeline-21425969342680b7a99ef9f999a96f06
2. https://www.europages.co.uk/en/search?isPserpFirst=1&q=winery+supplies
3. https://beautiful-soup-4.readthedocs.io/en/latest/#quick-start
4. https://pandas.pydata.org/docs/user_guide/index.html
5. W3schools
5. Stackoverflow
6. Youtube
7. Chat GPT
8. Gemini AI
9. Geeks for Geeks
  
